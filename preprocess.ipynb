{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d14a589-d9c2-497b-8b96-d19724396651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac1a66c-2480-4bfb-b5d4-41fe34e13fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.5.1+cu118\n",
      "CUDA 사용 가능 여부: True\n",
      "PyTorch에서 사용하는 CUDA 버전: 11.8\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"PyTorch에서 사용하는 CUDA 버전:\", torch.version.cuda)\n",
    "torch.set_default_device(\"cuda\")\n",
    "# GPU용 Generator 생성\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = torch.Generator(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd63a9a-2049-471e-ac93-0135bb5821f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 3200 \n",
      "Epoch 0 - 6400 \n",
      "Epoch 0 - 9600 \n",
      "Epoch 0 - 12800 \n",
      "Epoch 0 - 16000 \n",
      "Epoch 0 - 19200 \n",
      "Epoch 0 - 22400 \n",
      "Epoch 0 - 25600 \n",
      "Epoch 0 - 28800 \n",
      "Epoch 0 - 32000 \n",
      "Epoch 0 - 35200 \n",
      "Epoch 0 - 38400 \n",
      "Epoch 0 - 41600 \n",
      "Epoch 0 - 44800 \n",
      "Epoch 0 - 48000 \n",
      "Epoch 0 - 51200 \n",
      "Epoch 0 - 54400 \n",
      "***Epoch 0 Total Loss = 626.215508647263***\n",
      "Epoch 1 - 57600 \n",
      "Epoch 1 - 60800 \n",
      "Epoch 1 - 64000 \n",
      "Epoch 1 - 67200 \n",
      "Epoch 1 - 70400 \n",
      "Epoch 1 - 73600 \n",
      "Epoch 1 - 76800 \n",
      "Epoch 1 - 80000 \n",
      "Epoch 1 - 83200 \n",
      "Epoch 1 - 86400 \n",
      "Epoch 1 - 89600 \n",
      "Epoch 1 - 92800 \n",
      "Epoch 1 - 96000 \n",
      "Epoch 1 - 99200 \n",
      "Epoch 1 - 102400 \n",
      "Epoch 1 - 105600 \n",
      "Epoch 1 - 108800 \n",
      "***Epoch 1 Total Loss = 298.9963792655617***\n",
      "Epoch 2 - 112000 \n",
      "Epoch 2 - 115200 \n",
      "Epoch 2 - 118400 \n",
      "Epoch 2 - 121600 \n",
      "Epoch 2 - 124800 \n",
      "Epoch 2 - 128000 \n",
      "Epoch 2 - 131200 \n",
      "Epoch 2 - 134400 \n",
      "Epoch 2 - 137600 \n",
      "Epoch 2 - 140800 \n",
      "Epoch 2 - 144000 \n",
      "Epoch 2 - 147200 \n",
      "Epoch 2 - 150400 \n",
      "Epoch 2 - 153600 \n",
      "Epoch 2 - 156800 \n",
      "Epoch 2 - 160000 \n",
      "Epoch 2 - 163200 \n",
      "***Epoch 2 Total Loss = 199.28455619886518***\n",
      "Epoch 3 - 166400 \n",
      "Epoch 3 - 169600 \n",
      "Epoch 3 - 172800 \n",
      "Epoch 3 - 176000 \n",
      "Epoch 3 - 179200 \n",
      "Epoch 3 - 182400 \n",
      "Epoch 3 - 185600 \n",
      "Epoch 3 - 188800 \n",
      "Epoch 3 - 192000 \n",
      "Epoch 3 - 195200 \n",
      "Epoch 3 - 198400 \n",
      "Epoch 3 - 201600 \n",
      "Epoch 3 - 204800 \n",
      "Epoch 3 - 208000 \n",
      "Epoch 3 - 211200 \n",
      "Epoch 3 - 214400 \n",
      "Epoch 3 - 217600 \n",
      "***Epoch 3 Total Loss = 136.42525675264187***\n",
      "Epoch 4 - 220800 \n",
      "Epoch 4 - 224000 \n",
      "Epoch 4 - 227200 \n",
      "Epoch 4 - 230400 \n",
      "Epoch 4 - 233600 \n",
      "Epoch 4 - 236800 \n",
      "Epoch 4 - 240000 \n",
      "Epoch 4 - 243200 \n",
      "Epoch 4 - 246400 \n",
      "Epoch 4 - 249600 \n",
      "Epoch 4 - 252800 \n",
      "Epoch 4 - 256000 \n",
      "Epoch 4 - 259200 \n",
      "Epoch 4 - 262400 \n",
      "Epoch 4 - 265600 \n",
      "Epoch 4 - 268800 \n",
      "Epoch 4 - 272000 \n",
      "***Epoch 4 Total Loss = 89.03631048859097***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9296996550091321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 3200 \n",
      "Epoch 0 - 6400 \n",
      "Epoch 0 - 9600 \n",
      "Epoch 0 - 12800 \n",
      "Epoch 0 - 16000 \n",
      "Epoch 0 - 19200 \n",
      "Epoch 0 - 22400 \n",
      "Epoch 0 - 25600 \n",
      "Epoch 0 - 28800 \n",
      "Epoch 0 - 32000 \n",
      "Epoch 0 - 35200 \n",
      "Epoch 0 - 38400 \n",
      "Epoch 0 - 41600 \n",
      "Epoch 0 - 44800 \n",
      "Epoch 0 - 48000 \n",
      "Epoch 0 - 51200 \n",
      "Epoch 0 - 54400 \n",
      "***Epoch 0 Total Loss = 588.1846195012331***\n",
      "Epoch 1 - 57600 \n",
      "Epoch 1 - 60800 \n",
      "Epoch 1 - 64000 \n",
      "Epoch 1 - 67200 \n",
      "Epoch 1 - 70400 \n",
      "Epoch 1 - 73600 \n",
      "Epoch 1 - 76800 \n",
      "Epoch 1 - 80000 \n",
      "Epoch 1 - 83200 \n",
      "Epoch 1 - 86400 \n",
      "Epoch 1 - 89600 \n",
      "Epoch 1 - 92800 \n",
      "Epoch 1 - 96000 \n",
      "Epoch 1 - 99200 \n",
      "Epoch 1 - 102400 \n",
      "Epoch 1 - 105600 \n",
      "Epoch 1 - 108800 \n",
      "***Epoch 1 Total Loss = 275.3694657860324***\n",
      "Epoch 2 - 112000 \n",
      "Epoch 2 - 115200 \n",
      "Epoch 2 - 118400 \n",
      "Epoch 2 - 121600 \n",
      "Epoch 2 - 124800 \n",
      "Epoch 2 - 128000 \n",
      "Epoch 2 - 131200 \n",
      "Epoch 2 - 134400 \n",
      "Epoch 2 - 137600 \n",
      "Epoch 2 - 140800 \n",
      "Epoch 2 - 144000 \n",
      "Epoch 2 - 147200 \n",
      "Epoch 2 - 150400 \n",
      "Epoch 2 - 153600 \n",
      "Epoch 2 - 156800 \n",
      "Epoch 2 - 160000 \n",
      "Epoch 2 - 163200 \n",
      "***Epoch 2 Total Loss = 181.66572797624394***\n",
      "Epoch 3 - 166400 \n",
      "Epoch 3 - 169600 \n",
      "Epoch 3 - 172800 \n",
      "Epoch 3 - 176000 \n",
      "Epoch 3 - 179200 \n",
      "Epoch 3 - 182400 \n",
      "Epoch 3 - 185600 \n",
      "Epoch 3 - 188800 \n",
      "Epoch 3 - 192000 \n",
      "Epoch 3 - 195200 \n",
      "Epoch 3 - 198400 \n",
      "Epoch 3 - 201600 \n",
      "Epoch 3 - 204800 \n",
      "Epoch 3 - 208000 \n",
      "Epoch 3 - 211200 \n",
      "Epoch 3 - 214400 \n",
      "Epoch 3 - 217600 \n",
      "***Epoch 3 Total Loss = 122.96541691955645***\n",
      "Epoch 4 - 220800 \n",
      "Epoch 4 - 224000 \n",
      "Epoch 4 - 227200 \n",
      "Epoch 4 - 230400 \n",
      "Epoch 4 - 233600 \n",
      "Epoch 4 - 236800 \n",
      "Epoch 4 - 240000 \n",
      "Epoch 4 - 243200 \n",
      "Epoch 4 - 246400 \n",
      "Epoch 4 - 249600 \n",
      "Epoch 4 - 252800 \n",
      "Epoch 4 - 256000 \n",
      "Epoch 4 - 259200 \n",
      "Epoch 4 - 262400 \n",
      "Epoch 4 - 265600 \n",
      "Epoch 4 - 268800 \n",
      "Epoch 4 - 272000 \n",
      "***Epoch 4 Total Loss = 76.97233930637594***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9295981871068119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 3200 \n",
      "Epoch 0 - 6400 \n",
      "Epoch 0 - 9600 \n",
      "Epoch 0 - 12800 \n",
      "Epoch 0 - 16000 \n",
      "Epoch 0 - 19200 \n",
      "Epoch 0 - 22400 \n",
      "Epoch 0 - 25600 \n",
      "Epoch 0 - 28800 \n",
      "Epoch 0 - 32000 \n",
      "Epoch 0 - 35200 \n",
      "Epoch 0 - 38400 \n",
      "Epoch 0 - 41600 \n",
      "Epoch 0 - 44800 \n",
      "Epoch 0 - 48000 \n",
      "Epoch 0 - 51200 \n",
      "Epoch 0 - 54400 \n",
      "***Epoch 0 Total Loss = 591.0733722820878***\n",
      "Epoch 1 - 57600 \n",
      "Epoch 1 - 60800 \n",
      "Epoch 1 - 64000 \n",
      "Epoch 1 - 67200 \n",
      "Epoch 1 - 70400 \n",
      "Epoch 1 - 73600 \n",
      "Epoch 1 - 76800 \n",
      "Epoch 1 - 80000 \n",
      "Epoch 1 - 83200 \n",
      "Epoch 1 - 86400 \n",
      "Epoch 1 - 89600 \n",
      "Epoch 1 - 92800 \n",
      "Epoch 1 - 96000 \n",
      "Epoch 1 - 99200 \n",
      "Epoch 1 - 102400 \n",
      "Epoch 1 - 105600 \n",
      "Epoch 1 - 108800 \n",
      "***Epoch 1 Total Loss = 274.5867224931717***\n",
      "Epoch 2 - 112000 \n",
      "Epoch 2 - 115200 \n",
      "Epoch 2 - 118400 \n",
      "Epoch 2 - 121600 \n",
      "Epoch 2 - 124800 \n",
      "Epoch 2 - 128000 \n",
      "Epoch 2 - 131200 \n",
      "Epoch 2 - 134400 \n",
      "Epoch 2 - 137600 \n",
      "Epoch 2 - 140800 \n",
      "Epoch 2 - 144000 \n",
      "Epoch 2 - 147200 \n",
      "Epoch 2 - 150400 \n",
      "Epoch 2 - 153600 \n",
      "Epoch 2 - 156800 \n",
      "Epoch 2 - 160000 \n",
      "Epoch 2 - 163200 \n",
      "***Epoch 2 Total Loss = 177.14567974186502***\n",
      "Epoch 3 - 166400 \n",
      "Epoch 3 - 169600 \n",
      "Epoch 3 - 172800 \n",
      "Epoch 3 - 176000 \n",
      "Epoch 3 - 179200 \n",
      "Epoch 3 - 182400 \n",
      "Epoch 3 - 185600 \n",
      "Epoch 3 - 188800 \n",
      "Epoch 3 - 192000 \n",
      "Epoch 3 - 195200 \n",
      "Epoch 3 - 198400 \n",
      "Epoch 3 - 201600 \n",
      "Epoch 3 - 204800 \n",
      "Epoch 3 - 208000 \n",
      "Epoch 3 - 211200 \n",
      "Epoch 3 - 214400 \n",
      "Epoch 3 - 217600 \n",
      "***Epoch 3 Total Loss = 114.73670278582722***\n",
      "Epoch 4 - 220800 \n",
      "Epoch 4 - 224000 \n",
      "Epoch 4 - 227200 \n",
      "Epoch 4 - 230400 \n",
      "Epoch 4 - 233600 \n",
      "Epoch 4 - 236800 \n",
      "Epoch 4 - 240000 \n",
      "Epoch 4 - 243200 \n",
      "Epoch 4 - 246400 \n",
      "Epoch 4 - 249600 \n",
      "Epoch 4 - 252800 \n",
      "Epoch 4 - 256000 \n",
      "Epoch 4 - 259200 \n",
      "Epoch 4 - 262400 \n",
      "Epoch 4 - 265600 \n",
      "Epoch 4 - 268800 \n",
      "Epoch 4 - 272000 \n",
      "***Epoch 4 Total Loss = 75.2037215285236***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.929784211594399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 3200 \n",
      "Epoch 0 - 6400 \n",
      "Epoch 0 - 9600 \n",
      "Epoch 0 - 12800 \n",
      "Epoch 0 - 16000 \n",
      "Epoch 0 - 19200 \n",
      "Epoch 0 - 22400 \n",
      "Epoch 0 - 25600 \n",
      "Epoch 0 - 28800 \n",
      "Epoch 0 - 32000 \n",
      "Epoch 0 - 35200 \n",
      "Epoch 0 - 38400 \n",
      "Epoch 0 - 41600 \n",
      "Epoch 0 - 44800 \n",
      "Epoch 0 - 48000 \n",
      "Epoch 0 - 51200 \n",
      "Epoch 0 - 54400 \n",
      "***Epoch 0 Total Loss = 619.0780635774136***\n",
      "Epoch 1 - 57600 \n",
      "Epoch 1 - 60800 \n",
      "Epoch 1 - 64000 \n",
      "Epoch 1 - 67200 \n",
      "Epoch 1 - 70400 \n",
      "Epoch 1 - 73600 \n",
      "Epoch 1 - 76800 \n",
      "Epoch 1 - 80000 \n",
      "Epoch 1 - 83200 \n",
      "Epoch 1 - 86400 \n",
      "Epoch 1 - 89600 \n",
      "Epoch 1 - 92800 \n",
      "Epoch 1 - 96000 \n",
      "Epoch 1 - 99200 \n",
      "Epoch 1 - 102400 \n",
      "Epoch 1 - 105600 \n",
      "Epoch 1 - 108800 \n",
      "***Epoch 1 Total Loss = 294.6299608387053***\n",
      "Epoch 2 - 112000 \n",
      "Epoch 2 - 115200 \n",
      "Epoch 2 - 118400 \n",
      "Epoch 2 - 121600 \n",
      "Epoch 2 - 124800 \n",
      "Epoch 2 - 128000 \n",
      "Epoch 2 - 131200 \n",
      "Epoch 2 - 134400 \n",
      "Epoch 2 - 137600 \n",
      "Epoch 2 - 140800 \n",
      "Epoch 2 - 144000 \n",
      "Epoch 2 - 147200 \n",
      "Epoch 2 - 150400 \n",
      "Epoch 2 - 153600 \n",
      "Epoch 2 - 156800 \n",
      "Epoch 2 - 160000 \n",
      "Epoch 2 - 163200 \n",
      "***Epoch 2 Total Loss = 195.1045906036161***\n",
      "Epoch 3 - 166400 \n",
      "Epoch 3 - 169600 \n",
      "Epoch 3 - 172800 \n",
      "Epoch 3 - 176000 \n",
      "Epoch 3 - 179200 \n",
      "Epoch 3 - 182400 \n",
      "Epoch 3 - 185600 \n",
      "Epoch 3 - 188800 \n",
      "Epoch 3 - 192000 \n",
      "Epoch 3 - 195200 \n",
      "Epoch 3 - 198400 \n",
      "Epoch 3 - 201600 \n",
      "Epoch 3 - 204800 \n",
      "Epoch 3 - 208000 \n",
      "Epoch 3 - 211200 \n",
      "Epoch 3 - 214400 \n",
      "Epoch 3 - 217600 \n",
      "***Epoch 3 Total Loss = 134.34137792186812***\n",
      "Epoch 4 - 220800 \n",
      "Epoch 4 - 224000 \n",
      "Epoch 4 - 227200 \n",
      "Epoch 4 - 230400 \n",
      "Epoch 4 - 233600 \n",
      "Epoch 4 - 236800 \n",
      "Epoch 4 - 240000 \n",
      "Epoch 4 - 243200 \n",
      "Epoch 4 - 246400 \n",
      "Epoch 4 - 249600 \n",
      "Epoch 4 - 252800 \n",
      "Epoch 4 - 256000 \n",
      "Epoch 4 - 259200 \n",
      "Epoch 4 - 262400 \n",
      "Epoch 4 - 265600 \n",
      "Epoch 4 - 268800 \n",
      "Epoch 4 - 272000 \n",
      "***Epoch 4 Total Loss = 86.8538835652871***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9278732327673679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 3200 \n",
      "Epoch 0 - 6400 \n",
      "Epoch 0 - 9600 \n",
      "Epoch 0 - 12800 \n",
      "Epoch 0 - 16000 \n",
      "Epoch 0 - 19200 \n",
      "Epoch 0 - 22400 \n",
      "Epoch 0 - 25600 \n",
      "Epoch 0 - 28800 \n",
      "Epoch 0 - 32000 \n",
      "Epoch 0 - 35200 \n",
      "Epoch 0 - 38400 \n",
      "Epoch 0 - 41600 \n",
      "Epoch 0 - 44800 \n",
      "Epoch 0 - 48000 \n",
      "Epoch 0 - 51200 \n",
      "Epoch 0 - 54400 \n",
      "***Epoch 0 Total Loss = 571.5130968131125***\n",
      "Epoch 1 - 57600 \n",
      "Epoch 1 - 60800 \n",
      "Epoch 1 - 64000 \n",
      "Epoch 1 - 67200 \n",
      "Epoch 1 - 70400 \n",
      "Epoch 1 - 73600 \n",
      "Epoch 1 - 76800 \n",
      "Epoch 1 - 80000 \n",
      "Epoch 1 - 83200 \n",
      "Epoch 1 - 86400 \n",
      "Epoch 1 - 89600 \n",
      "Epoch 1 - 92800 \n",
      "Epoch 1 - 96000 \n",
      "Epoch 1 - 99200 \n",
      "Epoch 1 - 102400 \n",
      "Epoch 1 - 105600 \n",
      "Epoch 1 - 108800 \n",
      "***Epoch 1 Total Loss = 255.6906772106886***\n",
      "Epoch 2 - 112000 \n",
      "Epoch 2 - 115200 \n",
      "Epoch 2 - 118400 \n",
      "Epoch 2 - 121600 \n",
      "Epoch 2 - 124800 \n",
      "Epoch 2 - 128000 \n",
      "Epoch 2 - 131200 \n",
      "Epoch 2 - 134400 \n",
      "Epoch 2 - 137600 \n",
      "Epoch 2 - 140800 \n",
      "Epoch 2 - 144000 \n",
      "Epoch 2 - 147200 \n",
      "Epoch 2 - 150400 \n",
      "Epoch 2 - 153600 \n",
      "Epoch 2 - 156800 \n",
      "Epoch 2 - 160000 \n",
      "Epoch 2 - 163200 \n",
      "***Epoch 2 Total Loss = 162.77603857940994***\n",
      "Epoch 3 - 166400 \n",
      "Epoch 3 - 169600 \n",
      "Epoch 3 - 172800 \n",
      "Epoch 3 - 176000 \n",
      "Epoch 3 - 179200 \n",
      "Epoch 3 - 182400 \n",
      "Epoch 3 - 185600 \n",
      "Epoch 3 - 188800 \n",
      "Epoch 3 - 192000 \n",
      "Epoch 3 - 195200 \n",
      "Epoch 3 - 198400 \n",
      "Epoch 3 - 201600 \n",
      "Epoch 3 - 204800 \n",
      "Epoch 3 - 208000 \n",
      "Epoch 3 - 211200 \n",
      "Epoch 3 - 214400 \n",
      "Epoch 3 - 217600 \n",
      "***Epoch 3 Total Loss = 105.1837583045708***\n",
      "Epoch 4 - 220800 \n",
      "Epoch 4 - 224000 \n",
      "Epoch 4 - 227200 \n",
      "Epoch 4 - 230400 \n",
      "Epoch 4 - 233600 \n",
      "Epoch 4 - 236800 \n",
      "Epoch 4 - 240000 \n",
      "Epoch 4 - 243200 \n",
      "Epoch 4 - 246400 \n",
      "Epoch 4 - 249600 \n",
      "Epoch 4 - 252800 \n",
      "Epoch 4 - 256000 \n",
      "Epoch 4 - 259200 \n",
      "Epoch 4 - 262400 \n",
      "Epoch 4 - 265600 \n",
      "Epoch 4 - 268800 \n",
      "Epoch 4 - 272000 \n",
      "***Epoch 4 Total Loss = 67.24373033095617***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9251843333558818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 3200 \n",
      "Epoch 0 - 6400 \n",
      "Epoch 0 - 9600 \n",
      "Epoch 0 - 12800 \n",
      "Epoch 0 - 16000 \n",
      "Epoch 0 - 19200 \n",
      "Epoch 0 - 22400 \n",
      "Epoch 0 - 25600 \n",
      "Epoch 0 - 28800 \n",
      "Epoch 0 - 32000 \n",
      "Epoch 0 - 35200 \n",
      "Epoch 0 - 38400 \n",
      "Epoch 0 - 41600 \n",
      "Epoch 0 - 44800 \n",
      "Epoch 0 - 48000 \n",
      "Epoch 0 - 51200 \n",
      "Epoch 0 - 54400 \n",
      "***Epoch 0 Total Loss = 603.1028490625322***\n",
      "Epoch 1 - 57600 \n",
      "Epoch 1 - 60800 \n",
      "Epoch 1 - 64000 \n",
      "Epoch 1 - 67200 \n",
      "Epoch 1 - 70400 \n",
      "Epoch 1 - 73600 \n",
      "Epoch 1 - 76800 \n",
      "Epoch 1 - 80000 \n",
      "Epoch 1 - 83200 \n",
      "Epoch 1 - 86400 \n",
      "Epoch 1 - 89600 \n",
      "Epoch 1 - 92800 \n",
      "Epoch 1 - 96000 \n",
      "Epoch 1 - 99200 \n",
      "Epoch 1 - 102400 \n",
      "Epoch 1 - 105600 \n",
      "Epoch 1 - 108800 \n",
      "***Epoch 1 Total Loss = 267.69333369936794***\n",
      "Epoch 2 - 112000 \n",
      "Epoch 2 - 115200 \n",
      "Epoch 2 - 118400 \n",
      "Epoch 2 - 121600 \n",
      "Epoch 2 - 124800 \n",
      "Epoch 2 - 128000 \n",
      "Epoch 2 - 131200 \n",
      "Epoch 2 - 134400 \n",
      "Epoch 2 - 137600 \n",
      "Epoch 2 - 140800 \n",
      "Epoch 2 - 144000 \n",
      "Epoch 2 - 147200 \n",
      "Epoch 2 - 150400 \n",
      "Epoch 2 - 153600 \n",
      "Epoch 2 - 156800 \n",
      "Epoch 2 - 160000 \n",
      "Epoch 2 - 163200 \n",
      "***Epoch 2 Total Loss = 174.72589910589159***\n",
      "Epoch 3 - 166400 \n",
      "Epoch 3 - 169600 \n",
      "Epoch 3 - 172800 \n",
      "Epoch 3 - 176000 \n",
      "Epoch 3 - 179200 \n",
      "Epoch 3 - 182400 \n",
      "Epoch 3 - 185600 \n",
      "Epoch 3 - 188800 \n",
      "Epoch 3 - 192000 \n",
      "Epoch 3 - 195200 \n",
      "Epoch 3 - 198400 \n",
      "Epoch 3 - 201600 \n",
      "Epoch 3 - 204800 \n",
      "Epoch 3 - 208000 \n",
      "Epoch 3 - 211200 \n",
      "Epoch 3 - 214400 \n",
      "Epoch 3 - 217600 \n",
      "***Epoch 3 Total Loss = 110.27238272014074***\n",
      "Epoch 4 - 220800 \n",
      "Epoch 4 - 224000 \n",
      "Epoch 4 - 227200 \n",
      "Epoch 4 - 230400 \n",
      "Epoch 4 - 233600 \n",
      "Epoch 4 - 236800 \n",
      "Epoch 4 - 240000 \n",
      "Epoch 4 - 243200 \n",
      "Epoch 4 - 246400 \n",
      "Epoch 4 - 249600 \n",
      "Epoch 4 - 252800 \n",
      "Epoch 4 - 256000 \n",
      "Epoch 4 - 259200 \n",
      "Epoch 4 - 262400 \n",
      "Epoch 4 - 265600 \n",
      "Epoch 4 - 268800 \n",
      "Epoch 4 - 272000 \n",
      "***Epoch 4 Total Loss = 74.72873868455645***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9265372387201515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wwir9\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 3200 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels)\n\u001b[0;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 58\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     60\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\_tensor.py:572\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \n\u001b[0;32m    530\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m        used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\overrides.py:1717\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[1;32m-> 1717\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\utils\\_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    data = pd.read_csv(\"filtered_data/filtered_data\" + str(i) + \".csv\", sep=';', on_bad_lines='skip')\n",
    "    texts = data['text'].tolist()\n",
    "    labels = data['type'].tolist()\n",
    "    \n",
    "    # BERT 토크나이저\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    #11558, 35558, 1029, 48145\n",
    "    # 클래스별 가중치 정의\n",
    "    class_weights = torch.tensor([0.45, 0.15, 0.4])  # 클래스의 가중치\n",
    "    \n",
    "    # 데이터셋 정의\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return item\n",
    " \n",
    "    lr=5e-6\n",
    "    batch_size=32\n",
    "    num_labels=3\n",
    "    \n",
    "    dataset = CustomDataset(encodings, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    # 손실 함수\n",
    "    loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    sequence = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 학습\n",
    "    model.train()\n",
    "    for epoch in range(5):  # N번 반복 학습\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            sequence = sequence + 1\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "    \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if sequence % 100 == 0:\n",
    "                print(f\"Epoch {epoch} - {batch_size * sequence} \")\n",
    "        print(f\"***Epoch {epoch} Total Loss = {total_loss}***\")\n",
    "        \n",
    "    # 모델 가중치만 저장\n",
    "    torch.save(model.state_dict(), \"parameter/final_model_weights\" + str(i + 1) + \".pth\")\n",
    "\n",
    "    data = pd.read_csv(\"cleaned_data.csv\", sep=';', on_bad_lines='skip')\n",
    "    texts = data['text'].tolist()\n",
    "    labels = data['type'].tolist()\n",
    "    torch.cuda.empty_cache()  # PyTorch가 사용하지 않는 메모리 반환\n",
    "    \n",
    "    # 평가\n",
    "    model.eval()\n",
    "    test_texts = texts\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    total = len(test_texts)\n",
    "    \n",
    "    batch_size=32\n",
    "    \n",
    "    test_dataset = CustomDataset(test_encodings, labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    correct_indices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            # 배치 데이터 가져오기\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "            \n",
    "            # 모델 예측\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits # [batch_size, 1] -> [batch_size]\n",
    "            # 소프트맥스 확률 계산\n",
    "            probabilities = torch.softmax(logits, dim=-1)  # [batch_size, num_classes]\n",
    "            # 가장 큰 확률의 클래스 인덱스를 예측\n",
    "            predictions = torch.argmax(probabilities, dim=-1)\n",
    "            # 정확도 계산\n",
    "            correct += (predictions == labels.int()).sum().item()\n",
    "            \n",
    "            # 올바른 데이터의 인덱스 확인\n",
    "            batch_correct_indices = torch.where(predictions == labels.int())[0].tolist()\n",
    "    \n",
    "            # 전체 데이터셋 인덱스로 변환\n",
    "            global_indices = [batch_idx * batch_size + idx for idx in batch_correct_indices]\n",
    "            correct_indices.extend(global_indices)\n",
    "    \n",
    "    # 원본 데이터에서 올바른 데이터만 필터링\n",
    "    filtered_data = data.iloc[correct_indices]\n",
    "    \n",
    "    # CSV로 저장\n",
    "    csv_file_path = \"filtered_data/filtered_data\" + str(i + 1) +\".csv\"\n",
    "    filtered_data.to_csv(csv_file_path, sep=';', index=False)\n",
    "            \n",
    "    print(\"Accuracy: \", correct/total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f4a4e-b422-4d87-a7f9-5b58339bfade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa8351-5937-4fbd-bcee-96d7b373aa19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
